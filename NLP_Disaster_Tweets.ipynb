{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mexican-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preceding-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solar-bones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "first-utilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "annual-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data points =  7613\n",
      "number of test data points =  3263\n"
     ]
    }
   ],
   "source": [
    "print('number of training data points = ', len(train))\n",
    "print('number of test data points = ', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assumed-replacement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weird-smart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      "id          3263 non-null int64\n",
      "keyword     3237 non-null object\n",
      "location    2158 non-null object\n",
      "text        3263 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "according-coral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of tweets about real disasters =  0.4296597924602653\n",
      "percent of tweets not about real disasters =  0.5703402075397347\n"
     ]
    }
   ],
   "source": [
    "percent_real = train[train['target'] == 1]['target'].sum()/len(train)\n",
    "percent_fake = 1 - percent_real\n",
    "print('percent of tweets about real disasters = ', percent_real)\n",
    "print('percent of tweets not about real disasters = ', percent_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "native-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decimal-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(train.drop(['target'], axis=1), train['target'], test_size=0.30, \n",
    "                                                  random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "italic-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "X_train['text'] = X_train['text'].apply(lambda x: x.lower())\n",
    "X_dev['text'] = X_dev['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pharmaceutical-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all URLs\n",
    "import re\n",
    "X_train['text'] = X_train['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "X_dev['text'] = X_dev['text'].apply(lambda x: re.sub(r'http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "established-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation (including @)\n",
    "import string\n",
    "X_train['text'] = X_train['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "X_dev['text'] = X_dev['text'].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "future-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['text'].str.split()\n",
    "X_dev['text'] = X_dev['text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "communist-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all stopwords using the NLTK stopwords package\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "X_train['text'] = X_train['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "X_dev['text'] = X_dev['text'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "revised-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "forced-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Lemmatize/Stem text using the Snowball Stemmer\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# X_train['text_stemmed'] = X_train['text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# X_dev['text_stemmed'] = X_dev['text'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stock-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eeshakhanna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize text using the WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "X_train['text_stemmed'] = X_train['text'].apply(lambda x: [wnl.lemmatize(y) for y in x])\n",
    "X_dev['text_stemmed'] = X_dev['text'].apply(lambda x: [wnl.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "introductory-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text_stemmed'] = X_train['text'].apply(lambda x: [wnl.lemmatize(y, pos='v') for y in x])\n",
    "X_dev['text_stemmed'] = X_dev['text'].apply(lambda x: [wnl.lemmatize(y, pos='v') for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "introductory-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text_stemmed'] = X_train['text'].apply(lambda x: [wnl.lemmatize(y, pos='a') for y in x])\n",
    "X_dev['text_stemmed'] = X_dev['text'].apply(lambda x: [wnl.lemmatize(y, pos='a') for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "natural-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train['text_stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "controversial-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text_stemmed_str'] = X_train['text_stemmed'].apply(lambda x: \" \".join(x))\n",
    "X_dev['text_stemmed_str'] = X_dev['text_stemmed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "civic-lucas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_stemmed_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2707</td>\n",
       "      <td>3889</td>\n",
       "      <td>detonation</td>\n",
       "      <td>New York</td>\n",
       "      <td>[detonation, fashionable, mountaineering, elec...</td>\n",
       "      <td>[detonation, fashionable, mountaineering, elec...</td>\n",
       "      <td>detonation fashionable mountaineering electron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6479</td>\n",
       "      <td>9266</td>\n",
       "      <td>sunk</td>\n",
       "      <td>Cardiff, Wales</td>\n",
       "      <td>[benaffleck, respected, liked, talent, guess, ...</td>\n",
       "      <td>[benaffleck, respected, liked, talent, guess, ...</td>\n",
       "      <td>benaffleck respected liked talent guess stil h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4499</td>\n",
       "      <td>6396</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>Berlin - Germany</td>\n",
       "      <td>[lavapixcom, see, hurricane, guillermo, meteoe...</td>\n",
       "      <td>[lavapixcom, see, hurricane, guillermo, meteoe...</td>\n",
       "      <td>lavapixcom see hurricane guillermo meteoearth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7181</td>\n",
       "      <td>10290</td>\n",
       "      <td>weapon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fur, leather, coats, sprite, amp, weapon, cho...</td>\n",
       "      <td>[fur, leather, coats, sprite, amp, weapon, cho...</td>\n",
       "      <td>fur leather coats sprite amp weapon choice lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>6038</td>\n",
       "      <td>heat%20wave</td>\n",
       "      <td>USA</td>\n",
       "      <td>[heat, advisory, effect, 1, pm, 7, pm, thursda...</td>\n",
       "      <td>[heat, advisory, effect, 1, pm, 7, pm, thursda...</td>\n",
       "      <td>heat advisory effect 1 pm 7 pm thursday buildi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      keyword          location  \\\n",
       "2707   3889   detonation          New York   \n",
       "6479   9266         sunk    Cardiff, Wales   \n",
       "4499   6396    hurricane  Berlin - Germany   \n",
       "7181  10290       weapon               NaN   \n",
       "4250   6038  heat%20wave               USA   \n",
       "\n",
       "                                                   text  \\\n",
       "2707  [detonation, fashionable, mountaineering, elec...   \n",
       "6479  [benaffleck, respected, liked, talent, guess, ...   \n",
       "4499  [lavapixcom, see, hurricane, guillermo, meteoe...   \n",
       "7181  [fur, leather, coats, sprite, amp, weapon, cho...   \n",
       "4250  [heat, advisory, effect, 1, pm, 7, pm, thursda...   \n",
       "\n",
       "                                           text_stemmed  \\\n",
       "2707  [detonation, fashionable, mountaineering, elec...   \n",
       "6479  [benaffleck, respected, liked, talent, guess, ...   \n",
       "4499  [lavapixcom, see, hurricane, guillermo, meteoe...   \n",
       "7181  [fur, leather, coats, sprite, amp, weapon, cho...   \n",
       "4250  [heat, advisory, effect, 1, pm, 7, pm, thursda...   \n",
       "\n",
       "                                       text_stemmed_str  \n",
       "2707  detonation fashionable mountaineering electron...  \n",
       "6479  benaffleck respected liked talent guess stil h...  \n",
       "4499      lavapixcom see hurricane guillermo meteoearth  \n",
       "7181  fur leather coats sprite amp weapon choice lif...  \n",
       "4250  heat advisory effect 1 pm 7 pm thursday buildi...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-tribe",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "consolidated-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 948)\n",
      "(2284, 948)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "min_df_bow = 11\n",
    "count_vect = CountVectorizer(binary=True, min_df=min_df_bow, analyzer='word')\n",
    "X_train_vocab = count_vect.fit_transform(X_train['text_stemmed_str']).toarray()\n",
    "X_dev_vocab = count_vect.transform(X_dev['text_stemmed_str']).toarray()\n",
    "print(X_train_vocab.shape)\n",
    "print(X_dev_vocab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "grateful-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for the word \"smoke\":  752\n"
     ]
    }
   ],
   "source": [
    "print('Index for the word \"smoke\": ', count_vect.vocabulary_.get(u'smoke'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-centre",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-spencer",
   "metadata": {},
   "source": [
    "### No Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "operating-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eeshakhanna/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='none')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression(penalty='none')\n",
    "logmodel.fit(X_train_vocab,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "above-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logmodel.predict(X_dev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "wrong-transportation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of the model:  0.7123727905731119\n",
      "false positives  289\n",
      "false negatives  248\n"
     ]
    }
   ],
   "source": [
    "# Error metrics - F1 Score\n",
    "# Double check with sklearn metric and to print false positives\n",
    "def err_metric(CM):      \n",
    "    TN = CM.iloc[0,0]\n",
    "    FN = CM.iloc[1,0]\n",
    "    TP = CM.iloc[1,1]\n",
    "    FP = CM.iloc[0,1]\n",
    "    precision =(TP)/(TP+FP)\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    recall = (TP)/(TP+FN)\n",
    "    f1_score = 2*(( precision * recall)/( precision + recall))\n",
    "    print(\"f1 score of the model: \",f1_score)\n",
    "    print(\"false positives \", FP)\n",
    "    print(\"false negatives \", FN)\n",
    "\n",
    "confusion_matrix = pd.crosstab(predictions,y_dev)\n",
    "#confusion_matrix_nb = pd.crosstab(predictions_nb, y_dev)\n",
    "err_metric(confusion_matrix)\n",
    "#err_metric(confusion_matrix_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "occupational-radio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      1330\n",
      "           1       0.73      0.70      0.71       954\n",
      "\n",
      "    accuracy                           0.76      2284\n",
      "   macro avg       0.76      0.76      0.76      2284\n",
      "weighted avg       0.76      0.76      0.76      2284\n",
      "\n",
      "0.7123727905731119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_dev,predictions))\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sized-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.89      3012\n",
      "           1       0.88      0.81      0.84      2317\n",
      "\n",
      "    accuracy                           0.87      5329\n",
      "   macro avg       0.87      0.86      0.86      5329\n",
      "weighted avg       0.87      0.87      0.87      5329\n",
      "\n",
      "0.8417040358744395\n"
     ]
    }
   ],
   "source": [
    "predictions_train = logmodel.predict(X_train_vocab)\n",
    "print(classification_report(y_train,predictions_train))\n",
    "print(f1_score(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-works",
   "metadata": {},
   "source": [
    "### L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "light-going",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "logmodel_l1.fit(X_train_vocab,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beneficial-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_l1 = logmodel_l1.predict(X_dev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "surprising-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1330\n",
      "           1       0.79      0.68      0.73       954\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.77      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n",
      "0.727580372250423\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev,predictions_l1))\n",
    "print(f1_score(y_dev, predictions_l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "champion-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87      3012\n",
      "           1       0.88      0.75      0.81      2317\n",
      "\n",
      "    accuracy                           0.85      5329\n",
      "   macro avg       0.86      0.84      0.84      5329\n",
      "weighted avg       0.85      0.85      0.85      5329\n",
      "\n",
      "0.8122372723026623\n"
     ]
    }
   ],
   "source": [
    "predictions_train_l1 = logmodel_l1.predict(X_train_vocab)\n",
    "print(classification_report(y_train,predictions_train_l1))\n",
    "print(f1_score(y_train, predictions_train_l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-provision",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "becoming-armenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_l2 = LogisticRegression(penalty='l2')\n",
    "logmodel_l2.fit(X_train_vocab,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "loving-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_l2 = logmodel_l2.predict(X_dev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "distinct-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.83      1330\n",
      "           1       0.78      0.69      0.73       954\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.77      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n",
      "0.7295036252091466\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev,predictions_l2))\n",
    "print(f1_score(y_dev, predictions_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "exterior-illinois",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      3012\n",
      "           1       0.88      0.77      0.82      2317\n",
      "\n",
      "    accuracy                           0.85      5329\n",
      "   macro avg       0.86      0.84      0.85      5329\n",
      "weighted avg       0.86      0.85      0.85      5329\n",
      "\n",
      "0.8198614318706697\n"
     ]
    }
   ],
   "source": [
    "predictions_train_l2 = logmodel_l2.predict(X_train_vocab)\n",
    "print(classification_report(y_train,predictions_train_l2))\n",
    "print(f1_score(y_train, predictions_train_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-australian",
   "metadata": {},
   "source": [
    "### Print Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "valued-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = np.transpose(logmodel_l1.coef_).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "arbitrary-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = count_vect.vocabulary_\n",
    "df = pd.DataFrame.from_dict(data=vocab, orient='index')\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={0: \"count\", \"index\": \"word\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "silent-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coef'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "defensive-constraint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "molecular-official",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8971429665875177"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['coef'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "other-colonial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3859198026904607"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['coef'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deadly-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5% words with highest coefficients \n",
    "num_coef = int(5*(df.shape[0])/100)\n",
    "imp_words = df.nlargest(num_coef, 'coef')['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "appointed-airfare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402        picking\n",
       "224         caught\n",
       "859         twelve\n",
       "475         taking\n",
       "784          lives\n",
       "525        history\n",
       "33            must\n",
       "263       collapse\n",
       "765    obliterated\n",
       "528       pakistan\n",
       "213            new\n",
       "194         hijack\n",
       "420           kids\n",
       "372        failure\n",
       "193         entire\n",
       "288           past\n",
       "599      explosion\n",
       "442           east\n",
       "264          words\n",
       "877          green\n",
       "909         middle\n",
       "132            hes\n",
       "453       upheaval\n",
       "112           lets\n",
       "704        crushed\n",
       "836        tonight\n",
       "254             st\n",
       "238            via\n",
       "516          thats\n",
       "168            hey\n",
       "553          obama\n",
       "145          radio\n",
       "699        drought\n",
       "410           riot\n",
       "837         chance\n",
       "315     evacuation\n",
       "548          place\n",
       "136        weather\n",
       "212        reddits\n",
       "777        outside\n",
       "200     responders\n",
       "335        islamic\n",
       "803      wednesday\n",
       "551           hell\n",
       "351          whole\n",
       "746           ball\n",
       "779         madhya\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-workshop",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "searching-vienna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56520923 0.43479077]\n",
      "(2, 948)\n"
     ]
    }
   ],
   "source": [
    "# BNB implementation \n",
    "n = X_train_vocab.shape[0] # size of the dataset\n",
    "d = X_train_vocab.shape[1] # number of features in our dataset\n",
    "K = 2 # number of clases\n",
    "alpha = 1\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K):\n",
    "    X_k = X_train_vocab[y_train == k]\n",
    "    #psis[k] = np.mean(X_k, axis=0)\n",
    "    psis[k] = (np.sum(X_k, axis=0) + alpha) / (X_k.shape[0] + (2*alpha)) #laplace smoothing\n",
    "    #psis[k] = ((np.mean(X_k, axis=0) * X_k.shape[0]) + alpha)/(X_k.shape[0] + (2*alpha)) \n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "# print out the class proportions\n",
    "print(phis)\n",
    "print(psis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ordered-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y).\"\"\"\n",
    "    \n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "pharmaceutical-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb, logpyx = nb_predictions(X_dev_vocab, psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "assured-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb_train, logpyx_train = nb_predictions(X_train_vocab, psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "traditional-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83      1330\n",
      "           1       0.80      0.66      0.72       954\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.77      0.78      2284\n",
      "weighted avg       0.79      0.79      0.78      2284\n",
      "\n",
      "0.7223502304147464\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, predictions_nb))\n",
    "print(f1_score(y_dev, predictions_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "lucky-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85      3012\n",
      "           1       0.87      0.69      0.77      2317\n",
      "\n",
      "    accuracy                           0.82      5329\n",
      "   macro avg       0.83      0.80      0.81      5329\n",
      "weighted avg       0.83      0.82      0.82      5329\n",
      "\n",
      "0.7679730704496273\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, predictions_nb_train))\n",
    "print(f1_score(y_train, predictions_nb_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-pixel",
   "metadata": {},
   "source": [
    "# N-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "unlikely-inspiration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 1006)\n",
      "(2284, 1006)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "min_df_ngram = 12\n",
    "count_vect_ngram = CountVectorizer(binary=True, min_df=min_df_ngram, ngram_range=(1,2))\n",
    "\n",
    "X_train_vocab_ngram = count_vect_ngram.fit_transform(X_train['text_stemmed_str']).toarray()\n",
    "X_dev_vocab_ngram = count_vect_ngram.transform(X_dev['text_stemmed_str']).toarray()\n",
    "print(X_train_vocab_ngram.shape)\n",
    "print(X_dev_vocab_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "peaceful-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 1-grams =  870\n",
      "number of 2-grams =  136\n"
     ]
    }
   ],
   "source": [
    "# Print number of 1-grams and 2-grams\n",
    "no_one_grams = 0\n",
    "no_two_grams = 0\n",
    "vocab = count_vect_ngram.vocabulary_\n",
    "gram = vocab.keys()\n",
    "value_iterator = iter(gram)\n",
    "for i in range(len(vocab)):\n",
    "    val = next(value_iterator)\n",
    "    if ' ' in val:\n",
    "        no_two_grams = no_two_grams + 1\n",
    "    else:\n",
    "        no_one_grams = no_one_grams + 1\n",
    "print('number of 1-grams = ', no_one_grams)\n",
    "print('number of 2-grams = ', no_two_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "quality-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heat wave', 'refugio oil', 'oil spill', 'spill may', 'may costly', 'costly big', 'big projected', 'burning buildings', 'severe thunderstorm', '70 years']\n"
     ]
    }
   ],
   "source": [
    "# Print 10 2-grams\n",
    "two_grams = []\n",
    "count = 0\n",
    "gram2 = count_vect_ngram.vocabulary_.keys()\n",
    "value_iterator_2 = iter(gram2)\n",
    "while count < 10:\n",
    "    val = next(value_iterator_2)\n",
    "    if ' ' in val:\n",
    "        two_grams.append(val)\n",
    "        count = count + 1\n",
    "print(two_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-lodge",
   "metadata": {},
   "source": [
    "### Logistic Regression L1 (N-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "healthy-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_ngram = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "logmodel_ngram.fit(X_train_vocab_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "affiliated-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ngram = logmodel_ngram.predict(X_dev_vocab_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "turned-belfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.82      1330\n",
      "           1       0.78      0.67      0.72       954\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.78      0.77      0.77      2284\n",
      "weighted avg       0.79      0.79      0.78      2284\n",
      "\n",
      "0.7240990990990991\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, predictions_ngram))\n",
    "print(f1_score(y_dev, predictions_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "brazilian-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ngram_train = logmodel_ngram.predict(X_train_vocab_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bearing-carrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87      3012\n",
      "           1       0.88      0.75      0.81      2317\n",
      "\n",
      "    accuracy                           0.84      5329\n",
      "   macro avg       0.85      0.83      0.84      5329\n",
      "weighted avg       0.85      0.84      0.84      5329\n",
      "\n",
      "0.8069110436609853\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, predictions_ngram_train))\n",
    "print(f1_score(y_train, predictions_ngram_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-gross",
   "metadata": {},
   "source": [
    "### Naive Bayes (N-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "trained-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56520923 0.43479077]\n",
      "(2, 1006)\n"
     ]
    }
   ],
   "source": [
    "# BNB Implementation \n",
    "n_ngram = X_train_vocab_ngram.shape[0] # size of the dataset\n",
    "d_ngram = X_train_vocab_ngram.shape[1] # number of features in our dataset\n",
    "K_ngram = 2 # number of clases\n",
    "alpha_ngram = 1\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis_ngram = np.zeros([K_ngram, d_ngram])\n",
    "phis_ngram = np.zeros([K_ngram])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K_ngram):\n",
    "    X_k = X_train_vocab_ngram[y_train == k]\n",
    "    #psis_ngram[k] = np.mean(X_k, axis=0)\n",
    "    psis_ngram[k] = (np.sum(X_k, axis=0) + alpha_ngram) / (X_k.shape[0] + (2*alpha_ngram)) #laplace smoothing\n",
    "    #psis_ngram[k] = ((np.mean(X_k, axis=0) * X_k.shape[0]) + alpha_ngram) / (X_k.shape[0] + (2*alpha_ngram))\n",
    "    phis_ngram[k] = X_k.shape[0] / float(n_ngram)\n",
    "\n",
    "# print out the class proportions\n",
    "print(phis_ngram)\n",
    "print(psis_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "taken-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb_ngram, logpyx_ngram = nb_predictions(X_dev_vocab_ngram, psis_ngram, phis_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "international-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb_train_ngram, logpyx_train_ngram = nb_predictions(X_train_vocab_ngram, psis_ngram, phis_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "resident-enemy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83      1330\n",
      "           1       0.82      0.61      0.70       954\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.79      0.76      0.76      2284\n",
      "weighted avg       0.79      0.78      0.77      2284\n",
      "\n",
      "0.6995785671282361\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, predictions_nb_ngram))\n",
    "print(f1_score(y_dev, predictions_nb_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "small-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85      3012\n",
      "           1       0.89      0.64      0.74      2317\n",
      "\n",
      "    accuracy                           0.81      5329\n",
      "   macro avg       0.83      0.79      0.80      5329\n",
      "weighted avg       0.82      0.81      0.80      5329\n",
      "\n",
      "0.7428859229413247\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, predictions_nb_train_ngram))\n",
    "print(f1_score(y_train, predictions_nb_train_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "express-orbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "0.7123727905731119\n",
      "0.8417040358744395\n",
      "0.727580372250423\n",
      "0.8122372723026623\n",
      "0.7295036252091466\n",
      "0.8198614318706697\n",
      "0.7223502304147464\n",
      "0.7679730704496273\n"
     ]
    }
   ],
   "source": [
    "# Print results for quick comparison and plotting\n",
    "\n",
    "print(min_df_bow)\n",
    "\n",
    "print(f1_score(y_dev, predictions))\n",
    "print(f1_score(y_train, predictions_train))\n",
    "\n",
    "print(f1_score(y_dev, predictions_l1))\n",
    "print(f1_score(y_train, predictions_train_l1))\n",
    "\n",
    "print(f1_score(y_dev, predictions_l2))\n",
    "print(f1_score(y_train, predictions_train_l2))\n",
    "\n",
    "print(f1_score(y_dev, predictions_nb))\n",
    "print(f1_score(y_train, predictions_nb_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "civic-switzerland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "0.7240990990990991\n",
      "0.8069110436609853\n",
      "0.6995785671282361\n",
      "0.7428859229413247\n"
     ]
    }
   ],
   "source": [
    "# Print results for quick comparison and plotting\n",
    "\n",
    "print(min_df_ngram)\n",
    "\n",
    "print(f1_score(y_dev, predictions_ngram))\n",
    "print(f1_score(y_train, predictions_ngram_train))\n",
    "\n",
    "print(f1_score(y_dev, predictions_nb_ngram))\n",
    "print(f1_score(y_train, predictions_nb_train_ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-dressing",
   "metadata": {},
   "source": [
    "# Final for Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-vatican",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "architectural-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = train.drop(['target'], axis=1)\n",
    "y_train_final = train['target']\n",
    "X_test_final = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "weighted-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do all pre-processing steps on final train set\n",
    "X_train_final['text'] = X_train_final['text'].apply(lambda x: x.lower())\n",
    "X_train_final['text'] = X_train_final['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "X_train_final['text'] = X_train_final['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "X_train_final['text'] = X_train_final['text'].str.split()\n",
    "X_train_final['text'] = X_train_final['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "X_train_final['text_stemmed'] = X_train_final['text'].apply(lambda x: [wnl.lemmatize(y) for y in x])\n",
    "X_train_final['text_stemmed'] = X_train_final['text'].apply(lambda x: [wnl.lemmatize(y, pos='v') for y in x])\n",
    "X_train_final['text_stemmed'] = X_train_final['text'].apply(lambda x: [wnl.lemmatize(y, pos='a') for y in x])\n",
    "X_train_final['text_stemmed_str'] = X_train_final['text_stemmed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "wound-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do all pre-processing steps on final test set\n",
    "X_test_final['text'] = X_test_final['text'].apply(lambda x: x.lower())\n",
    "X_test_final['text'] = X_test_final['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "X_test_final['text'] = X_test_final['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "X_test_final['text'] = X_test_final['text'].str.split()\n",
    "X_test_final['text'] = X_test_final['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "X_test_final['text_stemmed'] = X_test_final['text'].apply(lambda x: [wnl.lemmatize(y) for y in x])\n",
    "X_test_final['text_stemmed'] = X_test_final['text'].apply(lambda x: [wnl.lemmatize(y, pos='v') for y in x])\n",
    "X_test_final['text_stemmed'] = X_test_final['text'].apply(lambda x: [wnl.lemmatize(y, pos='a') for y in x])\n",
    "X_test_final['text_stemmed_str'] = X_test_final['text_stemmed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-ottawa",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "rising-michigan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 1294)\n",
      "(3263, 1294)\n"
     ]
    }
   ],
   "source": [
    "min_df_bow_final = 11\n",
    "count_vect_final = CountVectorizer(binary=True, min_df=min_df_bow_final, analyzer='word')\n",
    "X_train_final_vocab = count_vect_final.fit_transform(X_train_final['text_stemmed_str']).toarray()\n",
    "X_test_final_vocab = count_vect_final.transform(X_test_final['text_stemmed_str']).toarray()\n",
    "print(X_train_final_vocab.shape)\n",
    "print(X_test_final_vocab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-stomach",
   "metadata": {},
   "source": [
    "### Logistic Regression (L1 Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "charitable-alaska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_final = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "logmodel_final.fit(X_train_final_vocab, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "spoken-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final = logmodel_final.predict(X_test_final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "imposed-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final_train = logmodel_final.predict(X_train_final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "guided-bruce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.87      4342\n",
      "           1       0.87      0.76      0.81      3271\n",
      "\n",
      "    accuracy                           0.85      7613\n",
      "   macro avg       0.85      0.84      0.84      7613\n",
      "weighted avg       0.85      0.85      0.85      7613\n",
      "\n",
      "0.8128674069235793\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_final, predictions_final_train))\n",
    "print(f1_score(y_train_final, predictions_final_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "practical-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "worst-canberra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_submit_lr = pd.concat([test_ids, pd.Series(predictions_final)], axis=1)\n",
    "kaggle_submit_lr = kaggle_submit_lr.rename(columns={0: \"target\"})\n",
    "kaggle_submit_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "automated-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submit_lr.to_csv('nlp_disaster_logreg_l1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-amazon",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "selective-velvet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57034021 0.42965979]\n",
      "(2, 1294)\n"
     ]
    }
   ],
   "source": [
    "n_final = X_train_final_vocab.shape[0] # size of the dataset\n",
    "d_final = X_train_final_vocab.shape[1] # number of features in our dataset\n",
    "K_final = 2 # number of clases\n",
    "alpha_final = 1\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis_final = np.zeros([K_final, d_final])\n",
    "phis_final = np.zeros([K_final])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K_final):\n",
    "    X_k = X_train_final_vocab[y_train_final == k]\n",
    "    #psis_ngram[k] = np.mean(X_k, axis=0)\n",
    "    psis_final[k] = (np.sum(X_k, axis=0) + alpha_final) / (X_k.shape[0] + (2*alpha_final)) #laplace smoothing\n",
    "    #psis_ngram[k] = ((np.mean(X_k, axis=0) * X_k.shape[0]) + alpha_ngram) / (X_k.shape[0] + (2*alpha_ngram))\n",
    "    phis_final[k] = X_k.shape[0] / float(n_final)\n",
    "\n",
    "# print out the class proportions\n",
    "print(phis_final)\n",
    "print(psis_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "intense-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb_final, logpyx_final = nb_predictions(X_test_final_vocab, psis_final, phis_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "incoming-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb_train_final, logpyx_train_final = nb_predictions(X_train_final_vocab, psis_final, phis_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "allied-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.85      4342\n",
      "           1       0.87      0.69      0.77      3271\n",
      "\n",
      "    accuracy                           0.82      7613\n",
      "   macro avg       0.83      0.80      0.81      7613\n",
      "weighted avg       0.83      0.82      0.82      7613\n",
      "\n",
      "0.7663934426229508\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_final, predictions_nb_train_final))\n",
    "print(f1_score(y_train_final, predictions_nb_train_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "motivated-columbus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       0\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       0\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_submit_nb = pd.concat([test_ids, pd.Series(predictions_nb_final)], axis=1)\n",
    "kaggle_submit_nb = kaggle_submit_nb.rename(columns={0: \"target\"})\n",
    "kaggle_submit_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "respected-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submit_nb.to_csv('nlp_disaster_naivebayes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-sheep",
   "metadata": {},
   "source": [
    "### Logistic Regression (L1) using N-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "nuclear-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df_ngram_final = 12\n",
    "count_vect_final_ngram = CountVectorizer(binary=True, min_df=min_df_ngram_final, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "personalized-martial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 1464)\n",
      "(3263, 1464)\n"
     ]
    }
   ],
   "source": [
    "X_train_vocab_ngram_final = count_vect_final_ngram.fit_transform(X_train_final['text_stemmed_str']).toarray()\n",
    "X_test_vocab_ngram_final = count_vect_final_ngram.transform(X_test_final['text_stemmed_str']).toarray()\n",
    "print(X_train_vocab_ngram_final.shape)\n",
    "print(X_test_vocab_ngram_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "excellent-canon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_ngram_final = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "logmodel_ngram_final.fit(X_train_vocab_ngram_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "exact-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ngram_final = logmodel_ngram_final.predict(X_test_vocab_ngram_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "marine-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ngram_train_final = logmodel_ngram_final.predict(X_train_vocab_ngram_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "instrumental-bonus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.87      4342\n",
      "           1       0.87      0.76      0.81      3271\n",
      "\n",
      "    accuracy                           0.85      7613\n",
      "   macro avg       0.85      0.84      0.84      7613\n",
      "weighted avg       0.85      0.85      0.85      7613\n",
      "\n",
      "0.8130718954248367\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_final, predictions_ngram_train_final))\n",
    "print(f1_score(y_train_final, predictions_ngram_train_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "organizational-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       0\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_submit_ngram = pd.concat([test_ids, pd.Series(predictions_ngram_final)], axis=1)\n",
    "kaggle_submit_ngram = kaggle_submit_ngram.rename(columns={0: \"target\"})\n",
    "kaggle_submit_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "advance-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submit_ngram.to_csv('nlp_disaster_logreg_l1_ngram.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
